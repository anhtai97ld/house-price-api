import pandas as pd
import numpy as np
import io
import joblib
from typing import List, Optional
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sklearn.impute import SimpleImputer

# Initialize FastAPI app
app = FastAPI(
    title="House Price Prediction API",
    description="API ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√† s·ª≠ d·ª•ng Polynomial Features + ElasticNet",
    version="1.0.0"
)

# --- LOAD MODEL V√Ä L·∫§Y DANH S√ÅCH FEATURE T·ª™ MODEL ---
try:
    model = joblib.load('house_price_model.pkl')
    print("‚úÖ Model loaded successfully!")
    
    # L·∫•y danh s√°ch t√™n feature m√† model ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán
    if hasattr(model, 'feature_names_in_'):
        MODEL_EXPECTED_FEATURES = list(model.feature_names_in_)
        print(f"‚úÖ Model expects {len(MODEL_EXPECTED_FEATURES)} features")
        print(f"üìã First 10 features: {MODEL_EXPECTED_FEATURES[:10]}")
    else:
        # Fallback: ƒê·ªãnh nghƒ©a manual n·∫øu c·∫ßn
        MODEL_EXPECTED_FEATURES = None
        print("‚ùå Warning: Model does not have 'feature_names_in_'")

except FileNotFoundError:
    print("‚ùå Model file not found! Please train and save model first.")
    model = None
    MODEL_EXPECTED_FEATURES = None

# --- PYDANTIC MODELS ---
class HouseFeatures(BaseModel):
    """Model ƒë·∫ßu v√†o cho prediction ƒë∆°n l·∫ª"""
    OverallQual: int
    GrLivArea: int
    TotalBsmtSF: int
    BsmtFinSF1: int
    GarageCars: int
    GarageArea: int
    FirstFlrSF: int  # S·∫Ω ƒë∆∞·ª£c convert th√†nh 1stFlrSF
    SecondFlrSF: int  # S·∫Ω ƒë∆∞·ª£c convert th√†nh 2ndFlrSF
    LotArea: int
    YearBuilt: int
    YearRemodAdd: int
    TotRmsAbvGrd: int
    
    class Config:
        schema_extra = {
            "example": {
                "OverallQual": 7,
                "GrLivArea": 1500,
                "TotalBsmtSF": 900,
                "BsmtFinSF1": 700,
                "GarageCars": 2,
                "GarageArea": 480,
                "FirstFlrSF": 800,
                "SecondFlrSF": 700,
                "LotArea": 8000,
                "YearBuilt": 2000,
                "YearRemodAdd": 2000,
                "TotRmsAbvGrd": 7
            }
        }

class PredictionResponse(BaseModel):
    """Model response cho prediction"""
    predicted_price: float
    formatted_price: str
    confidence: str
    model_version: str = "Polynomial Features + ElasticNet v1.0"

# --- HELPER FUNCTIONS ---
def create_features_for_prediction(input_data: pd.DataFrame):
    """T·∫°o c√°c features m·ªõi t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o, gi·ªëng nh∆∞ trong qu√° tr√¨nh training"""
    nam_hien_tai = 2025
    df_moi = input_data.copy()
    
    # üîç DEBUG: Ki·ªÉm tra NaN values
    print(f"üìä DataFrame shape: {df_moi.shape}")
    print(f"üîç NaN values found: {df_moi.isnull().sum().sum()}")
    if df_moi.isnull().sum().sum() > 0:
        print("‚ùå Columns with NaN:")
        nan_cols = df_moi.columns[df_moi.isnull().any()].tolist()
        for col in nan_cols:
            print(f"   - {col}: {df_moi[col].isnull().sum()} NaN values")
    
    # üõ†Ô∏è X·ª¨ L√ù MISSING VALUES TR∆Ø·ªöC KHI T·∫†O FEATURES
    # Thay th·∫ø c√°c gi√° tr·ªã string missing b·∫±ng NaN
    df_moi = df_moi.replace(['', 'NULL', 'N/A', 'null', 'None', 'na', 'NA'], np.nan)
    
    # Fill NaN values v·ªõi gi√° tr·ªã h·ª£p l√Ω
    numeric_columns = df_moi.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if df_moi[col].isnull().sum() > 0:
            if col in ['TotalBsmtSF', 'BsmtFinSF1', 'GarageArea', 'GarageCars']:
                # Basement v√† garage c√≥ th·ªÉ kh√¥ng c√≥ -> fill 0
                df_moi[col] = df_moi[col].fillna(0)
            elif col in ['YearRemodAdd']:
                # N·∫øu kh√¥ng c√≥ remodel -> d√πng YearBuilt
                df_moi[col] = df_moi[col].fillna(df_moi.get('YearBuilt', 2000))
            else:
                # C√°c c·ªôt kh√°c -> d√πng median
                df_moi[col] = df_moi[col].fillna(df_moi[col].median())
    
    print(f"‚úÖ After cleaning: {df_moi.isnull().sum().sum()} NaN values")

    # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ kh·ªõp v·ªõi training
    if 'FirstFlrSF' in df_moi.columns:
        df_moi['1stFlrSF'] = df_moi['FirstFlrSF']
        df_moi = df_moi.drop('FirstFlrSF', axis=1)
    if 'SecondFlrSF' in df_moi.columns:
        df_moi['2ndFlrSF'] = df_moi['SecondFlrSF']
        df_moi = df_moi.drop('SecondFlrSF', axis=1)

    # 1. T∆∞∆°ng t√°c gi·ªØa ch·∫•t l∆∞·ª£ng v√† di·ªán t√≠ch
    if 'OverallQual' in df_moi.columns and 'GrLivArea' in df_moi.columns:
        df_moi['ChatLuong_x_DienTich'] = df_moi['OverallQual'] * df_moi['GrLivArea']
    
    # 2. T·ª∑ l·ªá di·ªán t√≠ch basement
    if 'TotalBsmtSF' in df_moi.columns and 'GrLivArea' in df_moi.columns:
        df_moi['TyLe_Basement'] = df_moi['TotalBsmtSF'] / (df_moi['GrLivArea'] + 1)
    
    if 'BsmtFinSF1' in df_moi.columns and 'TotalBsmtSF' in df_moi.columns:
        df_moi['TyLe_Basement_HoanThien'] = df_moi['BsmtFinSF1'] / (df_moi['TotalBsmtSF'] + 1)
    
    # 3. Tu·ªïi nh√† v√† th·ªùi gian c·∫£i t·∫°o
    if 'YearBuilt' in df_moi.columns:
        df_moi['Tuoi_Nha'] = nam_hien_tai - df_moi['YearBuilt']
    
    if 'YearRemodAdd' in df_moi.columns and 'YearBuilt' in df_moi.columns:
        df_moi['Tuoi_CaiTao'] = nam_hien_tai - df_moi['YearRemodAdd']
        df_moi['Thoi_Gian_CaiTao'] = df_moi['YearRemodAdd'] - df_moi['YearBuilt']
    
    # 4. Di·ªán t√≠ch trung b√¨nh m·ªói ph√≤ng
    if 'GrLivArea' in df_moi.columns and 'TotRmsAbvGrd' in df_moi.columns:
        df_moi['DienTich_MoiPhong'] = df_moi['GrLivArea'] / (df_moi['TotRmsAbvGrd'] + 1)
    
    # 5. Ch·ªâ s·ªë garage
    if 'GarageArea' in df_moi.columns and 'GarageCars' in df_moi.columns:
        df_moi['DienTich_Garage_MoiXe'] = df_moi['GarageArea'] / (df_moi['GarageCars'] + 1)
    
    # 6. Features ph√¢n lo·∫°i
    if 'YearRemodAdd' in df_moi.columns and 'YearBuilt' in df_moi.columns:
        df_moi['Da_CaiTao'] = (df_moi['YearRemodAdd'] > df_moi['YearBuilt']).astype(int)
    
    if 'TotalBsmtSF' in df_moi.columns:
        df_moi['Co_Basement'] = (df_moi['TotalBsmtSF'] > 0).astype(int)
    
    if 'BsmtFinSF1' in df_moi.columns:
        df_moi['Co_Basement_HoanThien'] = (df_moi['BsmtFinSF1'] > 0).astype(int)
    
    # 7. Features t·ªïng h·ª£p
    if all(col in df_moi.columns for col in ['1stFlrSF', '2ndFlrSF']):
        df_moi['Tong_DienTich_Tang'] = df_moi['1stFlrSF'] + df_moi['2ndFlrSF']
    
    # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt l√† s·ªë
    df_numeric = df_moi.select_dtypes(include=[np.number])
    
    return df_numeric

def calculate_confidence(overall_qual: int, gr_liv_area: int, total_bsmt_sf: int = 0) -> str:
    """T√≠nh to√°n ƒë·ªô tin c·∫≠y c·ªßa prediction"""
    if overall_qual >= 8 and gr_liv_area >= 1500:
        return "High"
    elif overall_qual >= 6 and gr_liv_area >= 1000:
        return "Medium"
    else:
        return "Low"

# --- API ENDPOINTS ---
@app.get("/")
async def read_root():
    """Root endpoint"""
    return {
        "message": "üè† House Price Prediction API",
        "version": "1.0.0",
        "status": "active" if model is not None else "model_not_loaded",
        "endpoints": {
            "predict": "/predict",
            "predict_batch": "/predict-batch", 
            "upload_csv": "/upload-csv",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "features_available": MODEL_EXPECTED_FEATURES is not None
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_single_house(house: HouseFeatures):
    """D·ª± ƒëo√°n gi√° cho m·ªôt ng√¥i nh√†"""
    if model is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    if MODEL_EXPECTED_FEATURES is None:
        raise HTTPException(status_code=500, detail="Model features not identified")
    
    try:
        # Chuy·ªÉn ƒë·ªïi input th√†nh DataFrame
        input_dict = house.dict()
        df = pd.DataFrame([input_dict])
        
        # T·∫°o features m·ªõi
        df_processed = create_features_for_prediction(df)
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß features cho model
        for col in MODEL_EXPECTED_FEATURES:
            if col not in df_processed.columns:
                df_processed[col] = 0
        
        # L·ªçc v√† s·∫Øp x·∫øp features theo th·ª© t·ª± model mong ƒë·ª£i
        X_predict = df_processed[MODEL_EXPECTED_FEATURES]
        
        # D·ª± ƒëo√°n
        prediction = model.predict(X_predict)[0]
        
        # T√≠nh confidence
        confidence = calculate_confidence(
            house.OverallQual, 
            house.GrLivArea, 
            house.TotalBsmtSF
        )
        
        return PredictionResponse(
            predicted_price=float(prediction),
            formatted_price=f"${prediction:,.0f}",
            confidence=confidence
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Prediction error: {str(e)}")

@app.post("/predict-batch")
async def predict_batch_houses(houses: List[HouseFeatures]):
    """D·ª± ƒëo√°n gi√° cho nhi·ªÅu ng√¥i nh√†"""
    if model is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    if MODEL_EXPECTED_FEATURES is None:
        raise HTTPException(status_code=500, detail="Model features not identified")
    
    try:
        # Chuy·ªÉn ƒë·ªïi list th√†nh DataFrame
        input_data = [house.dict() for house in houses]
        df = pd.DataFrame(input_data)
        
        # T·∫°o features m·ªõi
        df_processed = create_features_for_prediction(df)
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß features cho model
        for col in MODEL_EXPECTED_FEATURES:
            if col not in df_processed.columns:
                df_processed[col] = 0
        
        # L·ªçc v√† s·∫Øp x·∫øp features theo th·ª© t·ª± model mong ƒë·ª£i
        X_predict = df_processed[MODEL_EXPECTED_FEATURES]
        
        # D·ª± ƒëo√°n
        predictions = model.predict(X_predict)
        
        # T·∫°o response
        results = []
        for i, (house, prediction) in enumerate(zip(houses, predictions)):
            confidence = calculate_confidence(
                house.OverallQual, 
                house.GrLivArea, 
                house.TotalBsmtSF
            )
            
            results.append({
                "index": i,
                "predicted_price": float(prediction),
                "formatted_price": f"${prediction:,.0f}",
                "confidence": confidence
            })
        
        return {"predictions": results, "total_count": len(results)}
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Batch prediction error: {str(e)}")

@app.post("/debug-csv")
async def debug_csv(file: UploadFile = File(...)):
    """Debug endpoint ƒë·ªÉ ki·ªÉm tra CSV file"""
    try:
        contents = await file.read()
        content_str = contents.decode('utf-8')
        
        # Th·ª≠ detect separator
        if ';' in content_str.split('\n')[0]:
            df = pd.read_csv(io.StringIO(content_str), sep=';')
            separator = "semicolon"
        else:
            df = pd.read_csv(io.StringIO(content_str))
            separator = "comma"
        
        # Th√¥ng tin chi ti·∫øt v·ªÅ file
        debug_info = {
            "file_info": {
                "filename": file.filename,
                "size_chars": len(content_str),
                "separator": separator,
                "shape": df.shape
            },
            "columns": list(df.columns),
            "data_types": df.dtypes.to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "sample_data": df.head(3).to_dict(),
            "numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(),
            "non_numeric_columns": df.select_dtypes(exclude=[np.number]).columns.tolist()
        }
        
        return debug_info
        
    except Exception as e:
        return {"error": str(e), "type": type(e).__name__}

@app.post("/upload-csv")
async def predict_from_csv(file: UploadFile = File(...)):
    """Upload CSV file v√† d·ª± ƒëo√°n gi√° cho t·∫•t c·∫£ nh√† trong file"""
    if model is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    if MODEL_EXPECTED_FEATURES is None:
        raise HTTPException(status_code=500, detail="Model feature names not identified")

    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="File must be CSV format")
    
    try:
        # ƒê·ªçc file CSV v·ªõi nhi·ªÅu options ƒë·ªÉ x·ª≠ l√Ω encoding
        contents = await file.read()
        content_str = contents.decode('utf-8')
        
        # üîç DEBUG: Ki·ªÉm tra n·ªôi dung file
        print(f"üìÑ File size: {len(content_str)} characters")
        print(f"üîç First 500 characters:")
        print(content_str[:500])
        
        # Th·ª≠ detect separator
        if ';' in content_str.split('\n')[0]:
            df = pd.read_csv(io.StringIO(content_str), sep=';')
            print("‚úÖ Using semicolon separator")
        else:
            df = pd.read_csv(io.StringIO(content_str))
            print("‚úÖ Using comma separator")
        
        print(f"üìä CSV loaded: {df.shape[0]} rows, {df.shape[1]} columns")
        print(f"üìã Columns: {list(df.columns)}")
        
        # üîç DEBUG: Ki·ªÉm tra data types v√† missing values
        print(f"üîç Data types:")
        print(df.dtypes)
        print(f"üîç Missing values per column:")
        print(df.isnull().sum())
        
        # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
        required_columns = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'BsmtFinSF1', 
                          'GarageCars', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'TotRmsAbvGrd']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise HTTPException(
                status_code=400, 
                detail=f"Missing required columns: {missing_columns}. Available columns: {list(df.columns)}"
            )
        
        # T·∫°o features m·ªõi
        print("üîß Creating features...")
        df_processed = create_features_for_prediction(df)
        
        # üõ†Ô∏è FINAL NaN CHECK - ƒê·∫£m b·∫£o kh√¥ng c√≤n NaN
        if df_processed.isnull().sum().sum() > 0:
            print("‚ö†Ô∏è Still have NaN values, applying final imputation...")
            # S·ª≠ d·ª•ng SimpleImputer ƒë·ªÉ x·ª≠ l√Ω NaN c√≤n l·∫°i
            imputer = SimpleImputer(strategy='median')
            df_processed_array = imputer.fit_transform(df_processed)
            df_processed = pd.DataFrame(df_processed_array, columns=df_processed.columns)
            print("‚úÖ Final imputation completed")
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß features cho model
        print("üîß Ensuring all required features...")
        for col in MODEL_EXPECTED_FEATURES:
            if col not in df_processed.columns:
                df_processed[col] = 0
                print(f"   + Added missing feature: {col}")
        
        # L·ªçc features theo model
        X_predict = df_processed[MODEL_EXPECTED_FEATURES]
        
        # üîç FINAL CHECK
        print(f"üìä Final prediction data shape: {X_predict.shape}")
        print(f"üîç Final NaN check: {X_predict.isnull().sum().sum()}")
        
        if X_predict.isnull().sum().sum() > 0:
            raise HTTPException(
                status_code=400, 
                detail=f"Still have NaN values after processing: {X_predict.isnull().sum().to_dict()}"
            )
        
        # D·ª± ƒëo√°n
        predictions = model.predict(X_predict)
        
        # Th√™m k·∫øt qu·∫£ v√†o DataFrame g·ªëc
        df['predicted_price'] = predictions
        df['formatted_price'] = df['predicted_price'].apply(lambda x: f"${x:,.0f}")
        df['confidence'] = df.apply(
            lambda row: calculate_confidence(
                row['OverallQual'], 
                row['GrLivArea'], 
                row.get('TotalBsmtSF', 0)
            ), 
            axis=1
        )
        
        # T·∫°o output CSV
        output = io.StringIO()
        df.to_csv(output, index=False)
        output.seek(0)
        
        return StreamingResponse(
            io.BytesIO(output.getvalue().encode()),
            media_type="text/csv",
            headers={"Content-Disposition": "attachment; filename=predictions.csv"}
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"CSV processing error: {str(e)}")

# --- MAIN ---
if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting House Price Prediction API...")
    print("üìã Available endpoints:")
    print("   - GET  /          : Root endpoint")
    print("   - GET  /health    : Health check")
    print("   - POST /predict   : Single prediction")
    print("   - POST /predict-batch : Batch prediction")
    print("   - POST /upload-csv : CSV upload prediction")
    print("üåê Access docs at: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)